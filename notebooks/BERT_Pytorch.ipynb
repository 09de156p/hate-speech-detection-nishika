{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP4Wgd2N073Fg2zOnrf8T6R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bf00b703c4684db6bd65e9536e1eb0ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07b5f501d14e41ef91bad2efdf9ea263","IPY_MODEL_611fcf7040194be88f04454f8877db72","IPY_MODEL_0aa53530d30b432691fdacaf40be01f5"],"layout":"IPY_MODEL_e6f5c7b043da4646b362c670b9143f52"}},"07b5f501d14e41ef91bad2efdf9ea263":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dce83a7ee9444146b9a3034f1ff87123","placeholder":"​","style":"IPY_MODEL_f9f6f993c7ff442f8df731e4d36d7d2b","value":""}},"611fcf7040194be88f04454f8877db72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8dfa4e719944c0a8527cf35d9383cf6","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4ff30c242884d9399cf75112aad6a48","value":0}},"0aa53530d30b432691fdacaf40be01f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fb76ac9d5bc485597f00f9de0021ac3","placeholder":"​","style":"IPY_MODEL_03247ba916a449c8961d29066b6f5324","value":" 0/0 [00:00&lt;?, ?it/s]"}},"e6f5c7b043da4646b362c670b9143f52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dce83a7ee9444146b9a3034f1ff87123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9f6f993c7ff442f8df731e4d36d7d2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8dfa4e719944c0a8527cf35d9383cf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a4ff30c242884d9399cf75112aad6a48":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fb76ac9d5bc485597f00f9de0021ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03247ba916a449c8961d29066b6f5324":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# ドライブをマウント\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJuUXAWLjPHv","executionInfo":{"status":"ok","timestamp":1665304483081,"user_tz":-540,"elapsed":4065,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"a020d9a1-2555-4f05-b6d2-a102d4347ffb"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"poJ0Tw9vb05Z","executionInfo":{"status":"ok","timestamp":1665304485998,"user_tz":-540,"elapsed":2925,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"54a1458f-a539-4509-fb44-ec76ad5bbbb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (5.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["!apt install aptitude swig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Yagq1iScFYw","executionInfo":{"status":"ok","timestamp":1665304489018,"user_tz":-540,"elapsed":3027,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"bff97ca5-c018-4560-f31b-6f8876eb423f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","aptitude is already the newest version (0.8.10-6ubuntu1).\n","swig is already the newest version (3.0.12-1).\n","0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"]}]},{"cell_type":"code","source":["!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ipLnh_gUcLJc","executionInfo":{"status":"ok","timestamp":1665304493575,"user_tz":-540,"elapsed":4579,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"95ad3486-07ae-4efa-b0ba-e6de0f8e7445"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["mecab is already installed at the requested version (0.996-5)\n","libmecab-dev is already installed at the requested version (0.996-5)\n","mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.12)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.20)\n","xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n","file is already installed at the requested version (1:5.32-2ubuntu0.4)\n","mecab is already installed at the requested version (0.996-5)\n","libmecab-dev is already installed at the requested version (0.996-5)\n","mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n","git is already installed at the requested version (1:2.17.1-1ubuntu0.12)\n","make is already installed at the requested version (4.1-9.1ubuntu1)\n","curl is already installed at the requested version (7.58.0-2ubuntu3.20)\n","xz-utils is already installed at the requested version (5.2.2-1.3ubuntu0.1)\n","file is already installed at the requested version (1:5.32-2ubuntu0.4)\n","No packages will be installed, upgraded, or removed.\n","0 packages upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n","Need to get 0 B of archives. After unpacking 0 B will be used.\n","                            \n"]}]},{"cell_type":"code","source":["!pip install mecab-python3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tV4N4ktecbrO","executionInfo":{"status":"ok","timestamp":1665304496001,"user_tz":-540,"elapsed":2445,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"c02a814c-91c2-4dad-8228-db46f9934b28"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import numpy as np\n","\n","# GPUが使えれば利用する設定\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"o6ftUJ76ccbk","executionInfo":{"status":"ok","timestamp":1665305175392,"user_tz":-540,"elapsed":614,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["dir = \"/content/drive/My Drive/hate-speech-detection-nishika/data/raw/\"\n","\n","train_raw = pd.read_csv(f\"{dir}train.csv\")\n","train = train_raw\n","train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"xPS1ysOaiG0g","executionInfo":{"status":"ok","timestamp":1665304498306,"user_tz":-540,"elapsed":24,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"4f9fffe7-ea5b-4dde-9cc9-efcb8f69a21d"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id       source                                               text  \\\n","0  80074aa43     news4vip                  まともに相手されてない人との関係なんて\\nそんな大事にするものか？   \n","1  6378fea6b  livejupiter             最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!   \n","2  c535f5613  livejupiter    日本人として生まれても無能な低学歴って分かったら日本人の権利剥奪して追放すべきやろ\\n甘えるな   \n","3  e76638295  livejupiter   よくよく思えば川上は配布にしたらとんでもなく有能だよな\\nガチャから引いたら圧倒的歓喜レベルやで   \n","4  51e4036bf     newsplus  押井は原作レイプの専門家だから\\n原作マンガの真意を誤解させることに関してはプロだが\\nそれ...   \n","\n","   label  \n","0      0  \n","1      0  \n","2      1  \n","3      0  \n","4      0  "],"text/html":["\n","  <div id=\"df-e295bf8f-d55a-4afc-9593-7cc70a380e24\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>source</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>80074aa43</td>\n","      <td>news4vip</td>\n","      <td>まともに相手されてない人との関係なんて\\nそんな大事にするものか？</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6378fea6b</td>\n","      <td>livejupiter</td>\n","      <td>最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>c535f5613</td>\n","      <td>livejupiter</td>\n","      <td>日本人として生まれても無能な低学歴って分かったら日本人の権利剥奪して追放すべきやろ\\n甘えるな</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>e76638295</td>\n","      <td>livejupiter</td>\n","      <td>よくよく思えば川上は配布にしたらとんでもなく有能だよな\\nガチャから引いたら圧倒的歓喜レベルやで</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>51e4036bf</td>\n","      <td>newsplus</td>\n","      <td>押井は原作レイプの専門家だから\\n原作マンガの真意を誤解させることに関してはプロだが\\nそれ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e295bf8f-d55a-4afc-9593-7cc70a380e24')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e295bf8f-d55a-4afc-9593-7cc70a380e24 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e295bf8f-d55a-4afc-9593-7cc70a380e24');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# データの抽出\n","sentences = train.text.values\n","labels = train.label.values"],"metadata":{"id":"KT4XAAP4isic","executionInfo":{"status":"ok","timestamp":1665304498307,"user_tz":-540,"elapsed":22,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# 1. BERT Tokenizerを用いて単語分割・IDへ変換\n","## Tokenizerの準備\n","from transformers import BertJapaneseTokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["bf00b703c4684db6bd65e9536e1eb0ed","07b5f501d14e41ef91bad2efdf9ea263","611fcf7040194be88f04454f8877db72","0aa53530d30b432691fdacaf40be01f5","e6f5c7b043da4646b362c670b9143f52","dce83a7ee9444146b9a3034f1ff87123","f9f6f993c7ff442f8df731e4d36d7d2b","a8dfa4e719944c0a8527cf35d9383cf6","a4ff30c242884d9399cf75112aad6a48","0fb76ac9d5bc485597f00f9de0021ac3","03247ba916a449c8961d29066b6f5324"]},"id":"XGEXznSKkGrM","executionInfo":{"status":"ok","timestamp":1665304498308,"user_tz":-540,"elapsed":22,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"2a6cb86d-c024-40cc-8ec3-98b7876a7650"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"output_type":"stream","name":"stdout","text":["Moving 0 files to the new cache system\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf00b703c4684db6bd65e9536e1eb0ed"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install fugashi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxWcgL6qlNRj","executionInfo":{"status":"ok","timestamp":1665304500850,"user_tz":-540,"elapsed":2557,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"b9b46aa8-e395-48ab-c62c-f5b38e63ab93"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: fugashi in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"]}]},{"cell_type":"code","source":["!pip install ipadic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RSOhXGBlhK3","executionInfo":{"status":"ok","timestamp":1665304503928,"user_tz":-540,"elapsed":3083,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"35e6e3c0-8e8a-4b21-f48e-1949037447c9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ipadic in /usr/local/lib/python3.7/dist-packages (1.0.0)\n"]}]},{"cell_type":"code","source":["# Load pre-trained tokenizer\n","tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"],"metadata":{"id":"L8XVl1dnka1N","executionInfo":{"status":"ok","timestamp":1665304505378,"user_tz":-540,"elapsed":1468,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["## テスト実行\n","for i in range(10):\n","  # 元文章\n","  print(' Original: ', sentences[i])\n","  # Tokenizer\n","  print('Tokenized: ', tokenizer.tokenize(sentences[i]))\n","  # Token-id\n","  print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[i])))\n","  print(\"=\" * 20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4GkbLFotk_Kt","executionInfo":{"status":"ok","timestamp":1665304505378,"user_tz":-540,"elapsed":9,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"87592e75-f199-48ee-f2a8-84ce835330a8"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":[" Original:  まともに相手されてない人との関係なんて\n","そんな大事にするものか？\n","Tokenized:  ['まとも', 'に', '相手', 'さ', 'れ', 'て', 'ない', '人', 'と', 'の', '関係', 'なんて', 'そんな', '大事', 'に', 'する', 'もの', 'か', '?']\n","Token IDs:  [23135, 7, 1879, 26, 20, 16, 80, 53, 13, 5, 633, 15060, 4799, 15872, 7, 34, 120, 29, 2935]\n","====================\n"," Original:  最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!\n","Tokenized:  ['最近', 'は', 'ア', '##ヘ', '##ア', '##ヘ', 'Q', '##S', 'マン', 'や', 'ない', '?', 'イ', '##イ', '!(', '・', '[UNK]', '・', ')', '##+', '1', '-', '0', '(', '・', 'A', '・', ')', 'イ', '##ク', '##ナイ', '!']\n","Token IDs:  [5233, 9, 43, 28962, 28483, 28962, 2984, 28583, 1490, 49, 80, 2935, 88, 28478, 21567, 35, 1, 35, 24, 29676, 17, 61, 518, 23, 35, 192, 35, 24, 88, 28488, 4114, 679]\n","====================\n"," Original:  日本人として生まれても無能な低学歴って分かったら日本人の権利剥奪して追放すべきやろ\n","甘えるな\n","Tokenized:  ['日本人', 'として', '生まれ', 'て', 'も', '無', '##能', 'な', '低', '学歴', 'って', '分かっ', 'たら', '日本人', 'の', '権利', '剥奪', 'し', 'て', '追放', 'す', 'べき', 'やろ', '甘', '##える', 'な']\n","Token IDs:  [2839, 50, 1115, 16, 28, 348, 28742, 18, 837, 26486, 6172, 8056, 3318, 2839, 5, 3410, 13430, 15, 16, 4856, 340, 1761, 22317, 5063, 358, 18]\n","====================\n"," Original:  よくよく思えば川上は配布にしたらとんでもなく有能だよな\n","ガチャから引いたら圧倒的歓喜レベルやで\n","Tokenized:  ['よく', 'よく', '思え', 'ば', '川上', 'は', '配布', 'に', 'し', 'たら', 'とん', '##でも', 'なく', '有能', 'だ', 'よ', 'な', 'ガ', '##チャ', 'から', '引い', 'たら', '圧倒的', '歓', '##喜', 'レベル', 'や', 'で']\n","Token IDs:  [1755, 1755, 17182, 312, 11925, 9, 7317, 7, 15, 3318, 14071, 3457, 332, 19627, 75, 54, 18, 444, 1540, 40, 10446, 3318, 7402, 7125, 29671, 2900, 49, 12]\n","====================\n"," Original:  押井は原作レイプの専門家だから\n","原作マンガの真意を誤解させることに関してはプロだが\n","それ以外には何も取り柄がない\n","Tokenized:  ['押', '##井', 'は', '原作', 'レイ', '##プ', 'の', '専門', '家', 'だ', 'から', '原作', 'マンガ', 'の', '真', '##意', 'を', '誤解', 'さ', 'せる', 'こと', 'に関して', 'は', 'プロ', 'だ', 'が', 'それ', '以外', 'に', 'は', '何', 'も', '取り', '##柄', 'が', 'ない']\n","Token IDs:  [2244, 29034, 9, 2461, 2995, 28526, 5, 1534, 167, 75, 40, 2461, 8433, 5, 841, 28804, 11, 10205, 26, 796, 45, 2174, 9, 285, 75, 14, 218, 1000, 7, 9, 1037, 28, 584, 29799, 14, 80]\n","====================\n"," Original:  マジレスすると野党に任せてドイツ化した場合、税金で難民を養うことになるね\n","生活保護どこらの話ではない\n","金の話をするなら視野を広くせんとなｗ\n","Tokenized:  ['マジ', 'レス', 'する', 'と', '野党', 'に', '任せ', 'て', 'ドイツ', '化', 'し', 'た', '場合', '、', '税金', 'で', '難民', 'を', '養', '##う', 'こと', 'に', 'なる', 'ね', '生活', '保護', 'どこ', '##ら', 'の', '話', 'で', 'は', 'ない', '金', 'の', '話', 'を', 'する', 'なら', '視野', 'を', '広く', 'せ', 'ん', 'と', 'な', 'w']\n","Token IDs:  [7557, 4451, 34, 13, 9627, 7, 12411, 16, 657, 237, 15, 10, 344, 6, 16695, 12, 10448, 11, 1745, 28489, 45, 7, 139, 1852, 1326, 1979, 5359, 28469, 5, 735, 12, 9, 80, 412, 5, 735, 11, 34, 737, 12886, 11, 2585, 191, 1058, 13, 18, 2835]\n","====================\n"," Original:  だから、『退治』なわけ。\n","殺されたとか、無実の人間みたいな言い方するな。\n","退治されただけだろうが。\n","Tokenized:  ['だから', '、', '『', '退治', '』', 'な', 'わけ', '。', '殺さ', 'れ', 'た', 'とか', '、', '無実', 'の', '人間', 'みたい', 'な', '言い方', 'する', 'な', '。', '退治', 'さ', 'れ', 'た', 'だけ', 'だろ', 'う', 'が', '。']\n","Token IDs:  [17957, 6, 63, 17945, 65, 18, 3133, 8, 6030, 20, 10, 10294, 6, 23092, 5, 1410, 17131, 18, 27206, 34, 18, 8, 17945, 26, 20, 10, 687, 3635, 205, 14, 8]\n","====================\n"," Original:  ワイD、柳の明治大と京田の日大を応援する予定\n","両方シード圏内に来て欲しいなあ\n","Tokenized:  ['ワイ', 'D', '、', '柳', 'の', '明治', '##大', 'と', '京', '##田', 'の', '日大', 'を', '応援', 'する', '予定', '両方', 'シード', '圏内', 'に', '来', 'て', '欲しい', 'な', '##あ']\n","Token IDs:  [9792, 300, 6, 3997, 5, 690, 28500, 13, 751, 28675, 5, 23084, 11, 6241, 34, 1484, 4588, 9435, 16689, 7, 1276, 16, 10928, 18, 28474]\n","====================\n"," Original:  キャラクターはこれといって有名なものはありません。なにか物事をとにかくキャラクターにするような感じです。イメージも人それぞれです。\n","Tokenized:  ['キャラクター', 'は', 'これ', 'と', 'いっ', 'て', '有名', 'な', 'もの', 'は', 'あり', 'ませ', 'ん', '。', 'なに', '##か', '物事', 'を', 'とにかく', 'キャラクター', 'に', 'する', 'よう', 'な', '感じ', 'です', '。', 'イメージ', 'も', '人', 'それぞれ', 'です', '。']\n","Token IDs:  [1480, 9, 171, 13, 1281, 16, 1948, 18, 120, 9, 130, 6769, 1058, 8, 14072, 28470, 20770, 11, 25523, 1480, 7, 34, 124, 18, 3415, 2992, 8, 3005, 28, 53, 976, 2992, 8]\n","====================\n"," Original:  都内の主要幹線道路の一角にいた百円で醤油の効いた海苔巻き餅を売ってくれたオジサン\n","あれは旨かった\n","Tokenized:  ['都内', 'の', '主要', '幹線', '道路', 'の', '一角', 'に', 'い', 'た', '百', '円', 'で', '醤油', 'の', '効', '##い', 'た', '海', '##苔', '巻き', '餅', 'を', '売っ', 'て', 'くれ', 'た', 'オ', '##ジ', '##サン', 'あれ', 'は', '旨', '##かっ', 'た']\n","Token IDs:  [11055, 5, 1877, 8338, 1305, 5, 10860, 7, 21, 10, 1625, 758, 12, 16943, 5, 1388, 28457, 10, 295, 31351, 3836, 16622, 11, 18544, 16, 4831, 10, 110, 28510, 1637, 2787, 9, 5553, 187, 10]\n","====================\n"]}]},{"cell_type":"code","source":["# 最大単語数の確認\n","max_len = []\n","# 1文づつ処理\n","for sent in sentences:\n","    # Tokenizeで分割\n","    token_words = tokenizer.tokenize(sent)\n","    # 文章数を取得してリストへ格納\n","    max_len.append(len(token_words))\n","# 最大の値を確認\n","print('最大単語数: ', max(max_len))\n","print('上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnXMbIo-ls0H","executionInfo":{"status":"ok","timestamp":1665304505886,"user_tz":-540,"elapsed":514,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"56735c56-4200-4977-e9ac-d11ece71be9d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["最大単語数:  91\n","上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数\n"]}]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","# 1文づつ処理\n","for sent in sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      \n","                        add_special_tokens = True, # Special Tokenの追加\n","                        max_length = 91,           # 文章の長さを固定（Padding/Trancatinating）\n","                        pad_to_max_length = True,# PADDINGで埋める\n","                        return_attention_mask = True,   # Attention maskの作成\n","                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n","                   )\n","\n","    # 単語IDを取得    \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # Attention　maskの取得\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# リストに入ったtensorを縦方向（dim=0）へ結合\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# tenosor型に変換\n","labels = torch.tensor(labels)\n","\n","# 確認\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1u_0gDg2l1it","executionInfo":{"status":"ok","timestamp":1665304508195,"user_tz":-540,"elapsed":2314,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"1e0b860a-dcf7-4ee6-9b10-369ebb6b780c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  まともに相手されてない人との関係なんて\n","そんな大事にするものか？\n","Token IDs: tensor([    2, 23135,     7,  1879,    26,    20,    16,    80,    53,    13,\n","            5,   633, 15060,  4799, 15872,     7,    34,   120,    29,  2935,\n","            3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# データセットクラスの作成\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# 90%地点のIDを取得\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","\n","# データセットを分割\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","print('訓練データ数：{}'.format(train_size))\n","print('検証データ数: {} '.format(val_size))\n","\n","# データローダーの作成\n","batch_size = 32\n","\n","# 訓練データローダー\n","train_dataloader = DataLoader(\n","            train_dataset,  \n","            sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化\n","            batch_size = batch_size\n","        )\n","\n","# 検証データローダー\n","validation_dataloader = DataLoader(\n","            val_dataset, \n","            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n","            batch_size = batch_size\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLZv_-63mOJw","executionInfo":{"status":"ok","timestamp":1665304508195,"user_tz":-540,"elapsed":9,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"cea21036-0066-4b81-e081-e4e8807e89df"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["訓練データ数：4730\n","検証データ数: 526 \n"]}]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# BertForSequenceClassification 学習済みモデルのロード\n","model = BertForSequenceClassification.from_pretrained(\n","    \"cl-tohoku/bert-base-japanese-whole-word-masking\", # 日本語Pre trainedモデルの指定\n","    num_labels = 2, # ラベル数（今回はBinayなので2、数値を増やせばマルチラベルも対応可）\n","     output_attentions = False, # アテンションベクトルを出力するか\n","     output_hidden_states = False, # 隠れ層を出力するか\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-NUPm9fmhNw","executionInfo":{"status":"ok","timestamp":1665304512350,"user_tz":-540,"elapsed":4160,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"1804ae8b-806c-444b-d0c2-bb9ad88efa08"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# モデルをGPUへ転送\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R56s1Kf1msm8","executionInfo":{"status":"ok","timestamp":1665304515192,"user_tz":-540,"elapsed":2859,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"07c962e2-503f-4a05-95ba-4dab8ac446d6"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 最適化手法の設定\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","\n","# 訓練パートの定義\n","def train(model):\n","    model.train() # 訓練モードで実行\n","    train_loss = 0\n","    for batch in train_dataloader: # train_dataloaderはword_id, mask, labelを出力する点に注意\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        optimizer.zero_grad()\n","        loss = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask, \n","                            labels=b_labels).loss # 戻り値とここを修正\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        train_loss += loss.item()\n","    return train_loss\n","\n","# テストパートの定義\n","def validation(model):\n","    model.eval() # 訓練モードをオフ\n","    val_loss = 0\n","    with torch.no_grad(): # 勾配を計算しない\n","        for batch in validation_dataloader:\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            with torch.no_grad():        \n","                loss = model(b_input_ids, \n","                                    token_type_ids=None, \n","                                    attention_mask=b_input_mask,\n","                                    labels=b_labels).loss # 戻り値とここを修正\n","            val_loss += loss.item()\n","    return val_loss"],"metadata":{"id":"zxhh0Yym0mdR","executionInfo":{"status":"ok","timestamp":1665304515192,"user_tz":-540,"elapsed":17,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# 学習の実行\n","max_epoch = 4\n","train_loss_ = []\n","test_loss_ = []\n","\n","for epoch in range(max_epoch):\n","    train_ = train(model)\n","    test_ = train(model)\n","    train_loss_.append(train_)\n","    test_loss_.append(test_)"],"metadata":{"id":"MMuRBETZsvV2","executionInfo":{"status":"ok","timestamp":1665305131331,"user_tz":-540,"elapsed":616155,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# 検証方法の確認（1バッチ分で計算ロジックに確認）\n","\n","model.eval()# 訓練モードをオフ\n","for batch in validation_dataloader:\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    with torch.no_grad():   \n","        # 学習済みモデルによる予測結果をpredsで取得     \n","        preds = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)"],"metadata":{"id":"F2pEedTpsvTY","executionInfo":{"status":"ok","timestamp":1665305134333,"user_tz":-540,"elapsed":3023,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["## 予測結果の確認\n","print(f'出力:{preds}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XnDysWSOsvQo","executionInfo":{"status":"ok","timestamp":1665305134337,"user_tz":-540,"elapsed":21,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"8b1d0083-ee36-4e23-dbd0-3388178aa890"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["出力:SequenceClassifierOutput(loss=None, logits=tensor([[ 6.1468, -4.6772],\n","        [ 6.0583, -4.7896],\n","        [ 5.8995, -4.4553],\n","        [ 6.1643, -4.9141],\n","        [ 5.8679, -4.3755],\n","        [ 5.9769, -4.7143],\n","        [ 5.6395, -4.1459],\n","        [ 6.0708, -4.7252],\n","        [ 5.7757, -4.3442],\n","        [ 6.1546, -4.7892],\n","        [ 5.9795, -4.5440],\n","        [ 6.1972, -4.8766],\n","        [-3.6924,  2.9827],\n","        [ 6.2017, -4.8550]], device='cuda:0'), hidden_states=None, attentions=None)\n"]}]},{"cell_type":"code","source":["# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n","logits_df = pd.DataFrame(preds[0].cpu().numpy(), columns=['logit_0', 'logit_1'])\n","## np.argmaxで大き方の値を取得\n","pred_df = pd.DataFrame(np.argmax(preds[0].cpu().numpy(), axis=1), columns=['pred_label'])\n","label_df = pd.DataFrame(b_labels.cpu().numpy(), columns=['true_label'])\n","\n","accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n","\n","accuracy_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"QckJUe06svNa","executionInfo":{"status":"ok","timestamp":1665305188337,"user_tz":-540,"elapsed":535,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"6eaa71fe-d483-4201-d0a6-efc91ba80d2f"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    logit_0   logit_1  pred_label  true_label\n","0  6.146763 -4.677162           0           0\n","1  6.058300 -4.789567           0           0\n","2  5.899467 -4.455338           0           0\n","3  6.164337 -4.914114           0           0\n","4  5.867900 -4.375484           0           0"],"text/html":["\n","  <div id=\"df-b7493b2b-560a-4658-86ad-b65b4c63506d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>logit_0</th>\n","      <th>logit_1</th>\n","      <th>pred_label</th>\n","      <th>true_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.146763</td>\n","      <td>-4.677162</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.058300</td>\n","      <td>-4.789567</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.899467</td>\n","      <td>-4.455338</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.164337</td>\n","      <td>-4.914114</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.867900</td>\n","      <td>-4.375484</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7493b2b-560a-4658-86ad-b65b4c63506d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b7493b2b-560a-4658-86ad-b65b4c63506d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b7493b2b-560a-4658-86ad-b65b4c63506d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["accuracy_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9ijU8QjMF4H","executionInfo":{"status":"ok","timestamp":1665309639231,"user_tz":-540,"elapsed":619,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"be7e0891-7d58-4ec0-f96f-5bfea38f7493"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(14, 4)"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["# 予測"],"metadata":{"id":"tPytvaO49jhp"}},{"cell_type":"code","source":["test_raw = pd.read_csv(f\"{dir}test.csv\")\n","test = test_raw\n","test.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"NHBs5Pwt9Zyy","executionInfo":{"status":"ok","timestamp":1665310384901,"user_tz":-540,"elapsed":621,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"c24ca60a-377c-4c24-a3e6-8ee53878dc35"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          id       source                                               text\n","0  001026808     news4vip  上でも言ったけどオタクレベルの知識求めてる訳じゃない\\nただ囲碁やります！って人が誰1人プロ...\n","1  00465ac96  livejupiter  たとえば、黒人なんかは、生物学的欠陥はないのに、文化的要因で、悪循環に陥り、実力をつけられず...\n","2  004674725  livejupiter  そうなんやろなあ色々と勿体ない感じしたわ\\n終わり方と黒幕キャラは好きやったで\\n\\nちなワ...\n","3  00474460f     news4vip  法的というか自治体ごとにバラバラの条例で定めてるだけだからな\\n普通の淫行条例だと「青少年に...\n","4  004a7525c     newsplus  別のジャーナリストの感想として言われてるので客観的な事実とは言えないけど、\\n現地は不測の事..."],"text/html":["\n","  <div id=\"df-26fb7824-302e-4574-9df1-5e305d4b8b72\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>source</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>001026808</td>\n","      <td>news4vip</td>\n","      <td>上でも言ったけどオタクレベルの知識求めてる訳じゃない\\nただ囲碁やります！って人が誰1人プロ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00465ac96</td>\n","      <td>livejupiter</td>\n","      <td>たとえば、黒人なんかは、生物学的欠陥はないのに、文化的要因で、悪循環に陥り、実力をつけられず...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>004674725</td>\n","      <td>livejupiter</td>\n","      <td>そうなんやろなあ色々と勿体ない感じしたわ\\n終わり方と黒幕キャラは好きやったで\\n\\nちなワ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00474460f</td>\n","      <td>news4vip</td>\n","      <td>法的というか自治体ごとにバラバラの条例で定めてるだけだからな\\n普通の淫行条例だと「青少年に...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>004a7525c</td>\n","      <td>newsplus</td>\n","      <td>別のジャーナリストの感想として言われてるので客観的な事実とは言えないけど、\\n現地は不測の事...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26fb7824-302e-4574-9df1-5e305d4b8b72')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-26fb7824-302e-4574-9df1-5e305d4b8b72 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-26fb7824-302e-4574-9df1-5e305d4b8b72');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# データの抽出\n","sentences_test = test.text.values\n","\n","input_ids_test = []\n","attention_masks_test = []\n","\n","# 1文づつ処理\n","for sent in sentences_test:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      \n","                        add_special_tokens = True, # Special Tokenの追加\n","                        max_length = 91,           # 文章の長さを固定（Padding/Trancatinating）\n","                        pad_to_max_length = True,# PADDINGで埋める\n","                        return_attention_mask = True,   # Attention maskの作成\n","                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n","                   )\n","\n","    # 単語IDを取得    \n","    input_ids_test.append(encoded_dict['input_ids'])\n","\n","    # Attention　maskの取得\n","    attention_masks_test.append(encoded_dict['attention_mask'])\n","\n","print(len(input_ids_test))\n","print(len(attention_masks_test))\n","\n","# リストに入ったtensorを縦方向（dim=0）へ結合\n","input_ids_test = torch.cat(input_ids_test, dim=0)\n","attention_masks_test = torch.cat(attention_masks_test, dim=0)\n","\n","# 確認\n","print('Original: ', sentences_test[0])\n","print('Token IDs:', input_ids_test[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v2q8LmpX-Y86","executionInfo":{"status":"ok","timestamp":1665311471169,"user_tz":-540,"elapsed":837,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"69996692-403e-4799-84e2-fb5b16850b4c"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["3223\n","3223\n","Original:  上でも言ったけどオタクレベルの知識求めてる訳じゃない\n","ただ囲碁やります！って人が誰1人プロ棋士わかりません、布石も知りませんなら申し訳ないけどお祈りかな\n","Token IDs: tensor([    2,   109,   962,  3083,    10, 11218, 25678,  2900,     5,  4125,\n","         2023,  7134,  3218,  4847,    80,   909, 11155,  4710,  2610,   679,\n","         6172,    53,    14,  3654,    17,    53,   285, 10400, 14428,  6769,\n","         1058,     6,  3432, 28922,    28,  4534,  6769,  1058,   737,  4482,\n","        29522,    80, 11218,    73, 30284, 28477,    29,    18,     3,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0])\n"]}]},{"cell_type":"code","source":["# データセットクラスの作成\n","test_dataset = TensorDataset(input_ids_test, attention_masks_test)\n","\n","# データローダー\n","test_dataloader = DataLoader(\n","            test_dataset, \n","            sampler = SequentialSampler(test_dataset), # 順番にデータを取得してバッチ化\n","            batch_size = batch_size\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UjR4C_cBC-wN","executionInfo":{"status":"ok","timestamp":1665311533023,"user_tz":-540,"elapsed":1280,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"09c4f1ee-be6f-4662-f8ed-d258f4abe484"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.sampler.SequentialSampler object at 0x7f6f1d8fa4d0>\n"]}]},{"cell_type":"code","source":["print(batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbTIJE0aQ5NQ","executionInfo":{"status":"ok","timestamp":1665311471170,"user_tz":-540,"elapsed":10,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"0f495073-9a5c-4fb3-cd34-dbfc5a7f1032"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["32\n"]}]},{"cell_type":"code","source":["list_wakaran = []\n","\n","# testデータを予測\n","model.eval()# 訓練モードをオフ\n","for batch in test_dataloader:\n","  \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    with torch.no_grad():   \n","        # 学習済みモデルによる予測結果をpredsで取得     \n","        preds = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","print(preds)\n","# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n","logits_df = pd.DataFrame(preds[0].cpu().numpy(), columns=['logit_0', 'logit_1'])\n","## np.argmaxで大き方の値を取得\n","pred_df = pd.DataFrame(np.argmax(preds[0].cpu().numpy(), axis=1), columns=['pred_label'])\n","\n","sub_df = pd.concat([logits_df, pred_df], axis=1)\n","\n","sub_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":622},"id":"sW51wKiZsvB9","executionInfo":{"status":"ok","timestamp":1665312342382,"user_tz":-540,"elapsed":20707,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"b37bcd93-b012-49e1-dd06-de79cad80676"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["SequenceClassifierOutput(loss=None, logits=tensor([[ 6.1020, -4.9324],\n","        [ 5.9201, -4.2099],\n","        [ 6.0720, -4.7944],\n","        [ 5.7130, -4.1942],\n","        [ 6.1347, -4.7427],\n","        [ 6.1977, -4.8221],\n","        [ 6.1566, -4.9264],\n","        [ 6.0806, -4.7799],\n","        [ 6.1437, -4.7584],\n","        [-3.6402,  3.1957],\n","        [-4.9136,  3.6865],\n","        [ 6.1652, -4.8344],\n","        [ 6.1122, -4.7379],\n","        [ 5.8553, -4.3990],\n","        [ 6.1630, -4.7428],\n","        [ 5.8369, -4.6909],\n","        [ 6.0296, -4.5642],\n","        [-3.9905,  2.9826],\n","        [ 6.1240, -4.8357],\n","        [ 6.0097, -4.4919],\n","        [ 6.0185, -4.3896],\n","        [ 6.1251, -4.6287],\n","        [ 6.1395, -4.8951]], device='cuda:0'), hidden_states=None, attentions=None)\n"]},{"output_type":"execute_result","data":{"text/plain":["    logit_0   logit_1  pred_label\n","0  6.101994 -4.932401           0\n","1  5.920081 -4.209941           0\n","2  6.072003 -4.794415           0\n","3  5.712966 -4.194177           0\n","4  6.134705 -4.742689           0"],"text/html":["\n","  <div id=\"df-34dcd2c6-21ad-42e2-a8e8-5aa1ccd92b7c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>logit_0</th>\n","      <th>logit_1</th>\n","      <th>pred_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.101994</td>\n","      <td>-4.932401</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.920081</td>\n","      <td>-4.209941</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.072003</td>\n","      <td>-4.794415</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5.712966</td>\n","      <td>-4.194177</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.134705</td>\n","      <td>-4.742689</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34dcd2c6-21ad-42e2-a8e8-5aa1ccd92b7c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-34dcd2c6-21ad-42e2-a8e8-5aa1ccd92b7c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-34dcd2c6-21ad-42e2-a8e8-5aa1ccd92b7c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["sub_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q_wmlzd-A-4O","executionInfo":{"status":"ok","timestamp":1665312345950,"user_tz":-540,"elapsed":522,"user":{"displayName":"kj nkgw","userId":"16512103549059864503"}},"outputId":"c5897764-422e-48ca-df00-92cefb3a05ae"},"execution_count":81,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(23, 3)"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":[],"metadata":{"id":"IZRmVJgKRjyl"},"execution_count":null,"outputs":[]}]}